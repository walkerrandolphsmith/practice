# Overview
Principal component analysis can reduce the dimensionality of dataset while retaining the most important information. When the dataset contains a set of highly correlated features it can be transformed into a set of uncorrelated features called principal components. The principal components are vectors that describe the most variance in the data and are assumed to be a linear combination of the features. The data is projected onto these vectors to reduce dimensionality. On one hand, the dataset becomes more interpretable when you can visualize the principal components in 2D or 3D space. On the other hand, the model's interpretability may decrease because it becomes less evident how the original feature space influences predictions.

# When should you consider PCA?
Linear regression assumes the features are not correlated and when this assumption is violated it becomes difficult to distinguish the individual impact of each feature due to multicollinearity, where multiple features are highly correlated. Principal components are reduced from a set of correlated features and are uncorrelated satisfying the assumption. Reducing the dimensionality of the dataset by reducing the number of features can also improve runtime performance of some models. For example, calculating the distance between data points by K-Nearest Neighbors or determining the hyperplane by Support Vector Machines (SVM) becomes more computationally expensive as the number of features grow. Research in [1] demonstrated the application of SVM without PCA was 26 times slower than the application of SVM with PCA while only experiencing a 2% reduction in accuracy. In addition to performance degradation, models like decision trees, support vector machines, and neural networks can suffer from overfitting when the number of features becomes large. Researchers in [2] demonstrated that the application of PCA was effective for crop classification while also mitigating the observation of overfitting.

# When should you avoid PCA?
PCA is not suitable for datasets with features with non-linear relationships. The principal components are vectors that are assumed to be a linear combination of the features. If the principal components cannot be represented as a linear combination of features via rotations, scaling, and translations then PCA is ineffective. If the dataset has features with linear relationships, but there are no subsets of features that are highly correlated then PCA will be ineffective. While PCA aims to preserve the most important information loss, reducing the feature space does cause some information loss. Interestingly, PCA might be a good choice for sparse data sets because the projection of data on the principal components may create a dense data set which could have implications on the space complexity of the model, or how much memory is needed.

# What type of data is good for PCA?
Datasets with large number of non-categorical features with linear relationships and many correlated features. Ideally a semi-dense or dense dataset is preferable to avoid increase in space complexity in some situations.

# What is the drawback of applying PCA as an agnostic model?

PCA preserves information from the features that explain the most variance in the data. If these features are not the same as the features that are most useful for prediction, then principal components will have less predictive power than a model that uses different features.

Researchers in [3] developed an effective method to predict real estate pricing using a neural network trained on a feature matrix that was derived from PCA. The original dataset contained seventeen features and PCA was applied to reduce the feature space by producing seven principal components that were linear combinations of highly correlated features. Seven was determined to be the number of principal components whose collective explanation of variance accounted for more than 89.7% of the variance. The reduced set of features was used in a neural network with a structure of 7 input neurons (accepting the 7 principal components) and 12 hidden neurons. The data was processed by these hidden neurons before being passed to a single output neuron. The real estate pricing domain contains many features that overlap such as square footage and number of bedrooms. Typically, the larger the home, measured in square footage, the more bedrooms the home will contain. This paper focused on the many overlapping factors affecting the accident risk of construction which is one component to real estate pricing.

References
[1] P. Yaswanthram and B. A. Sabarish, "Face Recognition Using Machine Learning Models - Comparative Analysis and impact of dimensionality reduction," 2022 IEEE Fourth International Conference on Advances in Electronics, Computers and Communications (ICAECC), Bengaluru, India, 2022, pp. 1-4, doi: 10.1109/ICAECC54045.2022.9716590.
[2] G. Shobana, S. N. Bushra, K. U. Maheswari and N. Subramanian, "Multivariate Classification of Dry Beans using Pipelined Dimensionality Reduction Technique," 2022 International Conference on Innovative Computing, Intelligent Communication and Smart Electrical Systems (ICSES), Chennai, India, 2022, pp. 1-6, doi: 10.1109/ICSES55317.2022.9914079.
[3] H. Shi, "Determination of Real Estate Price Based on Principal Component Analysis and Artificial Neural Networks," 2009 Second International Conference on Intelligent Computation Technology and Automation, Changsha, China, 2009, pp. 314-317, doi: 10.1109/ICICTA.2009.83.